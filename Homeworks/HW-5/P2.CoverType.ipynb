{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from string import split,strip\n",
    "\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cover Type\n",
    "\n",
    "Classify geographical locations according to their predicted tree cover:\n",
    "\n",
    "* **URL:** http://archive.ics.uci.edu/ml/datasets/Covertype\n",
    "* **Abstract:** Forest CoverType dataset\n",
    "* **Data Set Description:** http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Read the file into an RDD\n",
    "# If doing this on a real cluster, you need the file to be available on all nodes, ideally in HDFS.\n",
    "path='covtype/covtype.data'\n",
    "inputRDD=sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Making the problem binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data=inputRDD.map(lambda line: [float(x) for x in line.split(',')]).map(lambda V:LabeledPoint(1.0, V[:-1]) if V[-1] == 2.0 else LabeledPoint(0.0, V[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducing data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes: Data1=58714, trainingData=41176, testData=17538\n"
     ]
    }
   ],
   "source": [
    "#Data1=Data.sample(False,0.1).cache()\n",
    "Data1=Data.cache()\n",
    "(trainingData,testData)=Data.randomSplit([0.7,0.3],seed=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'test': 0.27403352719808416, 'train': 0.2751602875461434} 11 seconds\n",
      "3 {'test': 0.25424791880488085, 'train': 0.25155430347775404} 10 seconds\n",
      "6 {'test': 0.21849697799064888, 'train': 0.21177384884398678} 11 seconds\n",
      "10 {'test': 0.17253962823583077, 'train': 0.13969302506314357} 26 seconds\n",
      "{1: {'test': 0.27403352719808416, 'train': 0.2751602875461434}, 10: {'test': 0.17253962823583077, 'train': 0.13969302506314357}, 3: {'test': 0.25424791880488085, 'train': 0.25155430347775404}, 6: {'test': 0.21849697799064888, 'train': 0.21177384884398678}}\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "errors={}\n",
    "for depth in [20]:\n",
    "    start=time()\n",
    "    #model=GradientBoostedTrees.trainClassifier(##FILLIN to generate 10 trees ##)\n",
    "    model=GradientBoostedTrees.trainClassifier(trainingData, {}, numIterations=20, maxDepth=depth) # depth?\n",
    "    #print model.toDebugString()\n",
    "    errors[depth]={}\n",
    "    dataSets={'train':trainingData,'test':testData}\n",
    "    for name in dataSets.keys():  # Calculate errors on train and test sets\n",
    "        data=dataSets[name]\n",
    "        Predicted=model.predict(data.map(lambda x: x.features))\n",
    "        #LabelsAndPredictions=data. ### FILLIN ###\n",
    "        LabelsAndPredictions=data.map(lambda lp: lp.label).zip(Predicted)\n",
    "        Err = LabelsAndPredictions.filter(lambda (v,p):v != p).count()/float(data.count())\n",
    "        errors[depth][name]=Err\n",
    "    print depth,errors[depth]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'test': 0.4887102292165583, 'train': 0.48637555857781234} 2 seconds\n",
      "3 {'test': 0.2940472117687308, 'train': 0.29740625607149795} 2 seconds\n",
      "6 {'test': 0.2503136047439845, 'train': 0.2514085875267146} 3 seconds\n",
      "10 {'test': 0.23942296727106854, 'train': 0.23533126092869633} 5 seconds\n",
      "15 {'test': 0.20122020754932146, 'train': 0.18367495628521469} 7 seconds\n",
      "20 {'test': 0.18314517048694265, 'train': 0.1570089372449971} 9 seconds\n",
      "{1: {'test': 0.4887102292165583, 'train': 0.48637555857781234}, 3: {'test': 0.2940472117687308, 'train': 0.29740625607149795}, 6: {'test': 0.2503136047439845, 'train': 0.2514085875267146}, 10: {'test': 0.23942296727106854, 'train': 0.23533126092869633}, 15: {'test': 0.20122020754932146, 'train': 0.18367495628521469}, 20: {'test': 0.18314517048694265, 'train': 0.1570089372449971}}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "from time import time\n",
    "errors={}\n",
    "for depth in [30]:\n",
    "    start=time()\n",
    "    model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=20, maxDepth=depth) #FILLIN\n",
    "    #print model.toDebugString()\n",
    "    errors[depth]={}\n",
    "    dataSets={'train':trainingData,'test':testData}\n",
    "    for name in dataSets.keys():  # Calculate errors on train and test sets\n",
    "        ### FILLIN ###\n",
    "        data=dataSets[name]\n",
    "        Predicted = model.predict(data.map(lambda x: x.features))\n",
    "        LabelsAndPredictions=data.map(lambda lp: lp.label).zip(Predicted)\n",
    "        Err = LabelsAndPredictions.filter(lambda (v,p):v != p).count()/float(data.count())\n",
    "        errors[depth][name]=Err\n",
    "    print depth,errors[depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
